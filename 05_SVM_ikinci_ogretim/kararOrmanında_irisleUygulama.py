# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q-_1f9qoPA8JlBd-Yrfqg2Yn8fP0nFS0

RF (Random Forest, Rastgele Orman) algoritması uygulamasına örnek olarak, çokça duyulan ve kullanılan Iris Veri Seti üzerinden basit bir örnek ile çözümlemelerde bulunalım.

Iris Veri Seti 3 Iris bitki türüne (Iris Setosa, Iris Virginica ve Iris Versicolor) ait, her bir türden 50 örnek olmak üzere toplam 150 örnek sayısına sahip bir veri setidir.

Iris Veri Seti içerisinde;

Sınıflar (Türler);
Iris Setosa,
Iris Versicolor,
Iris Virginica.

Veri Özellikleri (Ortak Özellikler);
Sepal Uzunluk (cm),
Sepal Genişlik (cm),
Petal Genişliği (cm)
Petal Uzunluk (cm).
özellik ve değerleri bulunmaktadır.

Dilerseniz hızlıca analizimizi gerçekleştirmek için adımlarımızı uygulamaya başlayalım.
"""

#1-Gerçekleştireceğimiz analizler için kullanacağımız kütüphanelerimizi projemize dahil edelim;
#(Scikit-Learn, Pandas, Matplotlip, Seaborn)

from sklearn import datasets
import pandas as pan
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import matplotlib.pyplot as plot
import seaborn as sea

#datasets: Kullanacağımız veri kümesini çalışmaya dahil etmek için, (Iris)
#pan: İçeri aktardığımız veri kümesinin bir çok boyutlu olarak kullanacağımız veri çerçevesini (DataFrame) oluşturmak için,
#train_test_split: Eğitim ve Test kümesi işlemleri kullanımı için,
#RandomForestClassifier: Rastgele Orman modelini kullanmak için,
#metrics: Tahminlememizde doğruluk hesaplaması kullanmak için,
#plot: Görselleştirme kullanmak için,
#sea: Görselleştirme kullanmak için

#ilgili kütüphaneleri projemize aktarmaktayız.

#2-Çalışmamıza ilgili verilerimizi dahil edelim ve önizleme gerçekleştirelim;

iris_dataset = datasets.load_iris()
print(iris_dataset)

#Önceki çalışmamızda yerel dizin üzerinden ilgili verilerimizi projemize dahil etmiştik;
#Iris_Data = pd.read_csv(“D:\Yedekleme\Iris.csv”)

#3-Etiket türlerimizin ve özelliklerimizin önizlemesini gerçekleştirelim;
print(iris_dataset.target_names)
print(iris_dataset.feature_names)

#4-DataFrame yapımızı oluşturmadan verilerimize yönelik önizleme gerçekleştirelim; (TOP 10 Kayıt)
print(iris_dataset.data[0:10])
print(iris_dataset.target)

#5-Iris veri kümesi üzerinden bir DataFrame oluşturalım;
salt_data=pan.DataFrame({
       'sepal length':iris_dataset.data[:,0],
       'sepal width':iris_dataset.data[:,1],
       'petal length':iris_dataset.data[:,2],
       'petal width':iris_dataset.data[:,3],
       'species':iris_dataset.target
})

#6-Oluşturmuş olduğumuz DataFrame’in önzilemesini gerçekleştirelim;
salt_data.head()

#7-Veri kümemizi test ve eğitim/öğrenme kümeleri olarak ikiye bölelim-ayıralım; (Test-Train)
#Verileri; 35% Test, 65% Eğitim olarak ayıralım.
#test_size ile verilerin %kaçının test için kullanılacağını ifade belirleyebilmektesiniz. (Örneğimizde %35.)
#train_size ile verilerin %kaçının eğitim için kullanılacağını ifade belirleyebilmektesiniz.
#shuffle ile verilerin bölünmeden karıştırma uygulanıp-uygulanmayacağınız belirleyebilmektesiniz.
#random_state ile bölünmeden önce verilere-veriye uygulanan karıştırmayı kontrol edebilmektesiniz.

X=salt_data[['sepal length', 'sepal width', 'petal length', 'petal width']]
y=salt_data['species']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, shuffle=True)

#8-Veri kümemizi bölme-ayırma işleminden sonra modeli eğitim seti üzerinde eğitip, test seti üzerinde tahminler gerçekleştirelim;
#Verileri; 120 ağaç sayısı olarak ele alalım.
#n_estimators ile maksimum oylama veya tahmin ortalamalarını almadan önce inşa etmek istediğiniz ağaç sayısını belirleyebilmektesiniz.
#min_sample_leaf ile daha önce bir karar ağacı oluşturduysanız, minimum örnek yaprak boyutunun önemini anlayabilir ve sonrasında değer üzerinden sayısını belirleyebilmektesiniz.
#min_sample_split ile dahili düğümü bölmek için gereken minimum örnek sayısını belirleyebilmektesiniz.
#max_depth ile ağacın maksimum derinliğini belirleyebilmektesiniz.
#Kullanılmaz ise varsayılan olarak 0, tüm yapraklar saf olana veya tüm yapraklar min_samples_split örneklerinden daha azını içerene kadar düğümler genişletilmektedir.

clf=RandomForestClassifier(n_estimators=120)

clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)

#9-Eğitimden sonra, gerçek ve tahmin edilen değerleri kullanarak doğruluk değerini kontrol edelim;

print("Accuracy Value:",metrics.accuracy_score(y_test, y_pred))

#10-Eğitim işlemlerimizin tamamlanmasından sonra dilerseniz tahminlemenizi tek bir belirlediğiniz öğe içinde kontrol edebilirsiniz;

#sepal length = 1
#sepal width = 2
#petal length = 3
#petal width = 4
#değerleri için;
#[1]: Versicolor çiçeğini ifade etmektedir.

clf.predict([[1, 2, 3, 4]])

"""Şimdi analiz yapımızı biraz daha geliştirip, basit seviyede görselleştirme gerçekleştirelim."""

#11- 8.adım üzerindeki işlemlerimizi tekrar ederek analizimize başlayalım;

clf=RandomForestClassifier(n_estimators=120)
clf.fit(X_train,y_train)

#12-Rastgele Orman sınıflandırıcımızı bir seviye daha derinleştirip, eğitimde bulunalım;
#bootstrap ile ağaç oluştururken önyükleme örneklerinin kullanılıp kullanılmadığı kontrol edebilmektesiniz.
#Alt örnek boyutu; eğer bootstrap=True (varsayılan) ise max_samples parametresi ile kontrol edilmekte, aksi takdirde her bir ağacı oluşturmak için tüm veri seti kullanılmaktadır.
#class_weight ile formdaki sınıflarla ilişkili ağırlıkları belirleyebilmektesiniz.
#criterion ile bir bölünmenin kalitesini ölçme işlevini belirleyebilmektesiniz.
#Desteklenen kriterler Gini Safsızlığı/Kirliliği için “gini”, bilgi kazancı için “entropi” değerleri kullanılmaktadır.
#max_features ile bir düğümü ayırırken dikkate alınacak maksimum özellik sayısını belirleyebilmektesiniz.
#max_leaf_nodes ile maksimum kullanılacak yaprak sayısını belirleyebilmektesiniz.
#min_impurity_decrease ile safsızlık/kirlilik değerlerini belirleyebilmektesiniz.
#min_impurity_split ile safsızlık/kirlilik bölünmesi değerini belirleyebilmektesiniz.
#min_weight_fraction_leaf ile bir yaprak düğümde olması gereken ağırlıklar toplamının minimum ağırlıklı değerini belirleyebilmektesiniz.
#n_jobs ile paralel olarak çalıştırılacak iş sayısını belirleyebilmektesiniz.
#oob_score ile tahmin hatalarını hesaplayabilmektesiniz.
#verbose ile ayrıntı düzeyini belirleyebilmektesiniz.
#warm_start ile işlem sonucu dahilinde önceki uygun çözümü yeniden kullanmak yerine, yeni bir orman oluşturulmasını sağlayabilmektesiniz. (EK Parametreler ve bilgilendirmeleri.)

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
       max_depth=None, max_features='auto', max_leaf_nodes=None,
       min_impurity_decrease=0.0,
       min_samples_leaf=1, min_samples_split=0,
       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,
       oob_score=False, random_state=None, verbose=0,
       warm_start=False)

#13-Özelliklere yönelik önem puanlarını değerlendirelim;
#sort_values ile verilerinizi belirlemiş olduğunuz sütuna/parametreye göre sıralayabilmektesiniz.
#sort_values(by=’sıralanacak sütun’, ascending=False)

feature_imp = pan.Series(clf.feature_importances_,index=iris_dataset.feature_names).sort_values(ascending=False)
feature_imp

#14-Çıktı olarak elde ettiğimiz skor değerlerini görselleştirelim;

sea.barplot(x=feature_imp, y=feature_imp.index)
plot.xlabel('Feature Importance Score')
plot.ylabel('Features')
plot.title("Visualizing Important Features")
plot.show()

"""Skorlar üzerinden çıkarımla düşük öneme sahip olan parametreleri tahminleyicimizde göz ardı edip tekrar analizde bulunabiliriz."""

#15-Analizimizde kullanacağımız veri kümemizin parametreleri yeniden belirleyelim; (Test-Train)
X=salt_data[['petal length', 'petal width','sepal length']]
y=salt_data['species']
X
y

#16-Veri kümemizi test ve eğitim/öğrenme kümeleri olarak ikiye bölelim-ayıralım; (Test-Train)
#Verileri; 75% Test, 25% Eğitim olarak ayıralım.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75,shuffle=True, random_state=5)

#17-Modelimizi eğitim seti üzerinde eğitip, test seti üzerinde tahminler gerçekleştirelim;
#Verileri; 120 ağaç sayısı olarak ele alalım.

clf=RandomForestClassifier(n_estimators=120)
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)

#18-Son olarak modelimizin ne sıklıkla doğru olduğunu kontrol edelim; (Accuracy Score)

print("Accuracy Value:",metrics.accuracy_score(y_test, y_pred))

"""Önem değeri düşük olan özellikleri/parametreleri (Sepal Width) analizimizden çıkardıktan sonra doğruluk değerinin arttığını görebilirsiniz.*Yanıltıcı verilerin/parametrelerin analizden çıkarılması."""